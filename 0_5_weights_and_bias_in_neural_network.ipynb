{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsarfraz/Tiny-Machine-Learning/blob/main/0_5_weights_and_bias_in_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook I will be revisitng the single-layer neural network that I created in notebook 0.3\n",
        "\n",
        "I will now add a additional layer to this neural network (making it a two-layered network) and will be adding a additional neuron to the first layer."
      ],
      "metadata": {
        "id": "eSHF35MFLCx1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCm3x_4F8bvg"
      },
      "outputs": [],
      "source": [
        "### IMPORTING LIBRARIES ###\n",
        "import sys\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "### MAKING SURE THAT USER HAS TENSORFLOW 2 AND PYTHON 3 ###\n",
        "\n",
        "# This script requires TensorFlow 2 and Python 3.\n",
        "if tf.__version__.split('.')[0] != '2':\n",
        "    raise Exception((f\"The script is developed and tested for tensorflow 2. \"\n",
        "                     f\"Current version: {tf.__version__}\"))\n",
        "\n",
        "if sys.version_info.major < 3:\n",
        "    raise Exception((f\"The script is developed and tested for Python 3. \"\n",
        "                     f\"Current version: {sys.version_info.major}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Key Concepts\n",
        "\n",
        "Before I write the code of the single and double-layer neural networks I am going to define sone essential concepts/terms that will help in understanding how a neural net works.\n",
        "\n",
        "## Neurons\n",
        "\n",
        "Neurons are the basic units of a artificial neural network (ANN) or simulated neural network (SNN). Each neuron is connected to some/all of the neurons in the next layer. When inputs are transferred between neurons the weights are applied to the inputs along with the bias.\n",
        "\n",
        "## Weights\n",
        "\n",
        "Weights contol the signal (the connection strength) between two neurons. In other words, a weight decides how much influence the input would have on the neuron output.\n",
        "\n",
        "## Bias\n",
        "\n",
        "Biases are constant and always set to be 1 (this value can be changed). They are a additional input to the hidden and output layers but are not influenced by any layers behind them (they do not have any connections with the neurons in the previous layers). Biases are essentially constants associated with one neuron and their purpose is to ensure that when all the inputs, of the neuron, are zero that the neuron will still be activated.\n",
        "\n",
        "Biases are added to each individual neuron. I have included a illustration that shows how biases are added to each neuron in a 3-layered neural network:\n",
        "\n",
        "![illustration of how biases are added to each neuron in a neural net](images/0.5_bias_in_each_layer.jpg)\n",
        "\n",
        "## Linear Transformation\n",
        "\n",
        "Every neuron performs a linear transformation of its input using weights and biases. The linear transformation model is a equation of a straight line is slope-intercept form that looks like this:\n",
        "$$\n",
        "y= (weight*x) +bias\n",
        "$$\n",
        "\n",
        "It is important to ensure that a linear transformation is not the only thing that is used in each neuron because all layers in the neural network will behave in the same way since the composition of two linear functions is a linear function. A neural network will not be able to learn any complex task if linear transformations are only used in each neuron without anything else (such as activation functions).\n",
        "\n",
        "## Activation Functions\n",
        "\n",
        "**NOTE:** In this notebook, the neural networks that are created do not have activation functions which means that the linear transformation is only used in each neuron. I will be utlizing activation functions in future notebooks but wanted to define the concept here.\n",
        "\n",
        "Activation functions are a additional step to each layer and run after the linear transformation, of each neuron from the previous layer, occurs. An activation function decides whether a neuron should be activated (\"fired\"). In other words, deciding whether sending the neuron's input to the next layer of the neural network is important.\n",
        "\n",
        "There are many types of activation functions, some of them are:\n",
        "\n",
        "*  Binary Step Activation Functions\n",
        "*  Linear Activation Functions\n",
        "*  Sigmoid Activation Functions\n",
        "*  ReLU Activation Functions\n",
        "*  Softmax Activation Functions\n",
        "\n",
        "The image below illustrates how activation functions work. As you can see, the primary role of the activation function is to transform the summed weighted input from the neurons, in the previous layer, into a ouput value that can be fed into the next hidden layer or be used as final the neural networks final output.\n",
        "\n",
        "![illustration of activation functions](images/0.5_activation_function.jpg)\n"
      ],
      "metadata": {
        "id": "OroDzwqaJaRA"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATGD5hNP0QIB"
      },
      "source": [
        "# Retraining single layer network\n",
        "\n",
        "I am re-training the original single layer network that was created in notebook 0.3 and will display the ML model prediction when x =10. I will also display the learned weights of the single layer network.\n",
        "\n",
        "I am also re-sharing the illustration of the single layer neural network from notebook 0.3 to show how the neural netural works/functions\n",
        "\n",
        "![illustration of single layer neural network in code](images/0.3_Neural_Network_illustration.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLqOZPfC8gfb",
        "outputId": "52689a9f-57ff-4b3f-b7d7-be5d72feb49d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b17180dfd60>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "my_layer = keras.layers.Dense(units=1, input_shape=[1])\n",
        "model = tf.keras.Sequential([my_layer])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "model.fit(xs, ys, epochs=500, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtu5rJBA8o9M",
        "outputId": "4b79f666-616b-45a4-a139-b3774e665c4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 60ms/step\n",
            "[[18.97694]]\n"
          ]
        }
      ],
      "source": [
        "k = 250\n",
        " print(f\"User pressed the: {k}\")\n",
        "print(model.predict([10.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adfeHPyVu5Ib",
        "outputId": "9a039fb5-6a5e-4916-c392-9ae0d9b919ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1.996658]], dtype=float32), array([-0.98963875], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "print(my_layer.get_weights())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc18N_LA0QIE"
      },
      "source": [
        "#### Next lets train a 2-layer network and see what its prediction and weights are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT2ajN1fvAzJ",
        "outputId": "f51c6513-5df0-406f-b2ed-d3dd5947f762"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b170b699120>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "my_layer_1 = keras.layers.Dense(units=2, input_shape=[1])\n",
        "my_layer_2 = keras.layers.Dense(units=1)\n",
        "model = tf.keras.Sequential([my_layer_1, my_layer_2])\n",
        "model.compile(optimizer='sgd', loss='mean_squared_error')\n",
        "\n",
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)\n",
        "\n",
        "model.fit(xs, ys, epochs=500, verbose=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caJ7xfgPvRnV",
        "outputId": "037eb2c6-05ac-4e70-991c-df23434649c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 55ms/step\n",
            "[[19.]]\n"
          ]
        }
      ],
      "source": [
        "print(model.predict([10.0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfiM8J9RvUys",
        "outputId": "be7b2889-3f25-46c4-a668-63cb8db974ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1.5996603 , 0.04464365]], dtype=float32), array([-0.43972898,  0.17022227], dtype=float32)]\n",
            "[array([[ 1.2641675],\n",
            "       [-0.4981434]], dtype=float32), array([-0.35931277], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "print(my_layer_1.get_weights())\n",
        "print(my_layer_2.get_weights())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwL3-Ujg0QIG"
      },
      "source": [
        "#### Finally we can manually compute the output for our 2-layer network to better understand how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snvWM5NRvgg9",
        "outputId": "07b95198-2f05-4b82-86af-742ea23b9160"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.293332114815712\n",
            "11.552650034427643\n",
            "[18.999996]\n"
          ]
        }
      ],
      "source": [
        "value_to_predict = 10.0\n",
        "\n",
        "layer1_w1 = (my_layer_1.get_weights()[0][0][0])\n",
        "layer1_w2 = (my_layer_1.get_weights()[0][0][1])\n",
        "layer1_b1 = (my_layer_1.get_weights()[1][0])\n",
        "layer1_b2 = (my_layer_1.get_weights()[1][1])\n",
        "\n",
        "\n",
        "layer2_w1 = (my_layer_2.get_weights()[0][0])\n",
        "layer2_w2 = (my_layer_2.get_weights()[0][1])\n",
        "layer2_b = (my_layer_2.get_weights()[1][0])\n",
        "\n",
        "neuron1_output = (layer1_w1 * value_to_predict) + layer1_b1\n",
        "neuron2_output = (layer1_w2 * value_to_predict) + layer1_b2\n",
        "\n",
        "neuron3_output = (layer2_w1 * neuron1_output) + (layer2_w2 * neuron2_output) + layer2_b\n",
        "\n",
        "print(neuron1_output)\n",
        "print(neuron2_output)\n",
        "print(neuron3_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
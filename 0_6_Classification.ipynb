{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsarfraz/Tiny-Machine-Learning/blob/main/0_6_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook I will be talking about classification which is a supervised machine learning method where a model tries to predict the correct label/category of input data (images). In classification, a model is fully trained using training data and is then evaluated on test data to perform predictions on new unseen data.\n",
        "\n",
        "## Re-cap: Dataset types in ML model training:\n",
        "To re-cap from [notebook 0.4](https://github.com/hsarfraz/Tiny-Machine-Learning/blob/main/0_4_Graphing_Neural_Network_Accuracy.ipynb), here are definitions of the different types of datasets used during machine learning (ML) model training:\n",
        "\n",
        " Types of ML Datasets  | Definition\n",
        " ------------- | -------------\n",
        " Training Dataset  | The dataset that is initially used to fit/train the ML model\n",
        " Validation Dataset  | The dataset used during the training phase of the model and used to fine-tune the model's parameters and make it accurate  \n",
        " Test Dataset  | The dataset used after the ML model has been fully trained to assess the model's perfomance on completly new/unseen data\n",
        "\n",
        " *  **NOTE:** Using a validation dataset is optional when training a ML model, but it is highly recommended because it improves the models accuracy. Some experts say that a validation dataset is not required if the ML model has no hyperparameters (ML models that don't have tuning options)\n",
        "\n",
        "## Overview of Artificial Intelligence, Machine Learning, and Deep Learning:\n",
        "\n",
        "Classification is a supervised learning method. Supervised learning is a subcategory of artificial intelligence and machine learning. Before I start talking about classification in greater detail I wanted to include a diagram which highlights the difference between artificial intelligence, machine learning, and deep learning.\n",
        "\n",
        "<img src=\"images/0.6_AI_ML_DL_Overview.jpg\" width=\"700\">\n",
        "\n",
        "\n",
        "### Defining Supervised Learning, Classification, and Regression\n",
        "\n",
        "Here is a diagram which provides a detailed explanation of supervised learning (I will define the other machine learning methods in future notebooks):\n",
        "\n",
        "<img src=\"images/0.6_supervised_learning.jpg\" width=\"700\">\n",
        "\n",
        "### Connection between Supervised Learning and Deep Learning\n",
        "\n",
        "*  Neural Networks:\n",
        "  *  Neural networks are used in deep learning algorithms and they train data by mimicking the connection of nodes in the human brain ([notebook 0.3](https://github.com/hsarfraz/Tiny-Machine-Learning/blob/main/0_3_Neural_Network_Basics.ipynb) includes a illustration of a neural net)\n",
        "  *  To re-cap from [notebook 0.5](https://github.com/hsarfraz/Tiny-Machine-Learning/blob/main/0_5_weights_and_bias_in_neural_network.ipynb) each node takes in inputs, weights, a bias, and produces a output. If the output values exceeds the given threshold value (this is established by the activation function), then the node is activated or \"fires\" and passes the output to the next layer of the network.\n",
        "  *  Neural Networks learn this process through supervised learning (the process that I am referring to is how each node uses greadient decent to minimize the loss function with the use of inputs, weights, and bias). When the loss functions result is at or near zero (it has a slope of zero) then we can be confident in the model's accuracy to produce a correct output/prediction of the input data.\n",
        "\n",
        "### Difference of Classification and Regression Neural Network Architecture\n",
        "\n",
        "I have included another image which highlights the difference between the neural network architecture of a classification ML model and Regression model. The methodology of having multiple neurons in the output layer is often used in classification and the having one neuron in the output layer is often used in regression.\n",
        "\n",
        "<img src=\"images/0.6_classification_regression_NN_architecture.jpg\" width=\"700\">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MxCvsDBbDlAF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL2nKxjS4S_r"
      },
      "source": [
        "# Explaining the Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dpkllLt9knmt"
      },
      "outputs": [],
      "source": [
        "# Load the necessary libraries\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy\n",
        "\n",
        "# Checking to see if TensorFlow 2 and Python 3 since this script requires TensorFlow 2 and Python 3.\n",
        "if sys.version_info.major < 3:\n",
        "    raise Exception((f\"The script is developed and tested for Python 3. \"\n",
        "                     f\"Current version: {sys.version_info.major}\"))\n",
        "\n",
        "if tf.__version__.split('.')[0] != '2':\n",
        "    raise Exception((f\"The script is developed and tested for tensorflow 2. \"\n",
        "                     f\"Current version: {tf.__version__}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset `MNIST` and normalizing the images (Feature Scaling)\n",
        "\n",
        "The `MNIST` dataset is already built into Tensorflow. It is a dataset of images that are labelled from 0-9. The dataset is split into training images (60,000) and labels, as well as validation images (10,000) and labels that is used for testing/validation.\n",
        "\n",
        "### What is Feature Scaling?\n",
        "\n",
        "Feature scaling is a method used to normalise a range of independent variables or features in a dataset. Feature scaling is also known as data normalisation and is performed during the data preprocessing stage (which is an iterative process where raw data is transformed and cleaned into a usable format). For example, if you have two independent variables with different ranges (ex. salary and age) you would want to apply feature scaling to these variables so they can all be on the same range (such as being centred around 0 or from 0-1). The type of range in which you want to place the data depends on the scaling technique used. The two most common feature scaling techniques are **normalisation and standardisation** which I will define below:\n",
        "\n",
        "* **Normalisation (aka min-max scaling or min-max normalization)**: It is the simplest feature scaling method and rescales the independent variable feature range to [0,1].\n",
        "  * The general normalisation formula is given below (the **max(x)** and **min(x)** are the feature's maximum and minimum values):\n",
        "  $$\n",
        "  x'=\\dfrac{x-min(x)}{max(x)-min(x)}\n",
        "  $$\n",
        "  * If you would like to would like to specify your interval to be in any [a,b] range then you can specify the range using the formula below:\n",
        "  $$\n",
        "  x'=a+\\dfrac{(x-min(x))(b-a)}{max(x)-min(x)}\n",
        "  $$\n",
        "* **Standardisation (aka feature standardisation)**: Makes the data values, in each feature, have a mean of 0 and unit variance. The formula uses the features distributed mean and standard deviation and calculates the new data point with this formula ( $\\overline{x}$ is the average of the feature vector and $\\sigma$ is the standard deviation of the feature vector):\n",
        "\n",
        " $$\n",
        " xâ€™ = \\dfrac{x-\\overline{x}}{\\sigma}\n",
        " $$\n",
        "\n",
        "### When to use normalisation or standardisation?\n",
        "\n",
        "When building a machine learning pipeline it is important to consider whether you are going to normalise or standardise your dataset features. I have highlighted some situations when it is best to use each feature scaling method.\n",
        "\n",
        "* **Normalisation**: Used when the data distribution does not follow the Gaussian distribution. It is also useful in algorithms that do not assume the data distribution like K-Nearest Neighbors (a classification algorithm which is under supervised learning). Another popular use case of normalisation is when an algorithm requires the data to be on a 0-1 scale. For example, in image processing it is essential to normalise the pixel intensity to fit within a certain range (the range of pixels is from 0-255 for the RGB colour range)\n",
        "\n",
        "* **Standardisation**: Used when the data follows a Gaussian Distribution (although this is not always the case). Since standardisation does not have a bounding range any outlets in the data will not be affected by standardisation. Standardisation is useful in clustering analysis (an unsupervised learning algorithm), to compare the similarities found between features based on certain distance measures. Another use case is during principal components analysis when individuals are interested in the components that maximise the data variance (something which min-max scaling does not present)\n",
        "\n"
      ],
      "metadata": {
        "id": "KPU2DpkbvvQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### LOADING MNIST DATASET\n",
        "data = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (val_images, val_labels) = data.load_data()\n",
        "\n",
        "print(\"NOTE: I am printing out the max. and min. range of each image to show why I am dividing each image pixel by 255 for normalisation\")\n",
        "print()\n",
        "\n",
        "### GETTING THE MAX AND MIN PIXCEL VALUES WITHOUT USING NUMPY (SLICING METHOD)\n",
        "print(\"\\033[1mFinding the max and min pixel using slicing\\033[0m\")\n",
        "print(\"Here is the minimum pixel value in the training image dataset: \", training_images[..., 1].min())\n",
        "print(\"Here is the maximum pixel value in the training image dataset: \", training_images[..., 1].max())\n",
        "\n",
        "print()\n",
        "### GETTING THE MAX AND MIN PIXCEL VALUES USING NUMPY (NUMPY METHOD)\n",
        "print(\"\\033[1mFinding the max and min pixel using numpy\\033[0m\")\n",
        "print(\"Here is the minimum pixel value in the training image dataset: \", numpy.amin(training_images))\n",
        "print(\"Here is the minimum pixel value in the training image dataset: \", numpy.amax(training_images))\n",
        "\n",
        "### FEATURE SCALING (NORMALIZATION)\n",
        "training_images  = training_images / 255.0\n",
        "val_images = val_images / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG2dJWhpv3BX",
        "outputId": "8cb5d7af-e853-4f8b-9e1c-3618d1ff8e72"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NOTE: I am printing out the max. and min. range of each image to show why I am dividing each image pixel by 255 for normalisation\n",
            "\n",
            "\u001b[1mFinding the max and min pixel using slicing\u001b[0m\n",
            "Here is the minimum pixel value in the training image dataset:  0\n",
            "Here is the maximum pixel value in the training image dataset:  255\n",
            "\n",
            "\u001b[1mFinding the max and min pixel using numpy\u001b[0m\n",
            "Here is the minimum pixel value in the training image dataset:  0\n",
            "Here is the minimum pixel value in the training image dataset:  255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start with a simple neural network for MNIST\n",
        "Note that there are 2 layers, one with 20 neurons, and one with 10.\n",
        "\n",
        "The 10-neuron layer is our final layer because we have 10 classes we want to classify.\n",
        "\n",
        "Train this, and you should see it get about 98% accuracy"
      ],
      "metadata": {
        "id": "qNL05RtntkOp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrQ-DeQtybXZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=20, validation_data=(val_images, val_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN92xbwK4i_S"
      },
      "source": [
        "## Examine the test data\n",
        "\n",
        "Using model.evaluate, you can get metrics for a test set. In this case we only have a training set and a validation set, so we can try it out with the validation set. The accuracy will be slightly lower, at maybe 96%.  This is because the model hasn't previously seen this data and may not be fully generalized for all data. Still it's a pretty good score.\n",
        "\n",
        "You can also predict images, and compare against their actual label. The [0] image in the set is a number 7, and here you can see that neuron 7 has a 9.9e-1 (99%+) probability, so it got it right!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rzit5Te-4lT6"
      },
      "outputs": [],
      "source": [
        "\n",
        "model.evaluate(val_images, val_labels)\n",
        "\n",
        "classifications = model.predict(val_images)\n",
        "print(classifications[0])\n",
        "print(val_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LkJGAiI5Cr3"
      },
      "source": [
        "## Modify to inspect learned values\n",
        "\n",
        "This code is identical, except that the layers are named prior to adding to the sequential. This allows us to inspect their learned parameters later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyyJ3RMYpFXR"
      },
      "outputs": [],
      "source": [
        "data = tf.keras.datasets.mnist\n",
        "\n",
        "(training_images, training_labels), (val_images, val_labels) = data.load_data()\n",
        "\n",
        "training_images  = training_images / 255.0\n",
        "val_images = val_images / 255.0\n",
        "layer_1 = tf.keras.layers.Dense(20, activation=tf.nn.relu)\n",
        "layer_2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    layer_1,\n",
        "                                    layer_2])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=20)\n",
        "\n",
        "model.evaluate(val_images, val_labels)\n",
        "\n",
        "classifications = model.predict(val_images)\n",
        "print(classifications[0])\n",
        "print(val_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pNYFF935PlE"
      },
      "source": [
        "# Inspect weights\n",
        "\n",
        "If you print layer_1.get_weights(), you'll see a lot of data. Let's unpack it. First, there are 2 arrays in the result, so let's look at the first one. In particular let's look at its size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QACivjNKxFWW"
      },
      "outputs": [],
      "source": [
        "print(layer_1.get_weights()[0].size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqpHrDyp5acs"
      },
      "source": [
        "The above code should print 15680. Why?\n",
        "\n",
        "Recall that there are 20 neurons in the first layer.\n",
        "\n",
        "Recall also that the images are 28x28, which is 784.\n",
        "\n",
        "If you multiply 784 x 20 you get 15680.\n",
        "\n",
        "So...this layer has 20 neurons, and each neuron learns a W parameter for each pixel. So instead of y=Mx+c, we have\n",
        "y=M1X1+M2X2+M3X3+....+M784X784+C in every neuron!\n",
        "\n",
        "Every pixel has a weight in every neuron. Those weights are multiplied by the pixel value, summed up, and given a bias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdXrHDEw6ACm"
      },
      "outputs": [],
      "source": [
        "print(layer_1.get_weights()[1].size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIOZ7rJy6Eg1"
      },
      "source": [
        "The above code will give you 20 -- the get_weights()[1] contains the biases for each of the 20 neurons in this layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyFKpzxN6T-N"
      },
      "source": [
        "## Inspecting layer 2\n",
        "\n",
        "Now let's look at layer 2. Printing the get_weights will give us 2 lists, the first a list of weights for the 10 neurons, and the second a list of biases for the 10 neurons\n",
        "\n",
        "Let's look first at the weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9P_PVwXyKXJ"
      },
      "outputs": [],
      "source": [
        "print(layer_2.get_weights()[0].size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daah2gq56fdb"
      },
      "source": [
        "This should return 200. Again, consider why?\n",
        "\n",
        "There are 10 neurons in this layer, but there are 20 neurons in the previous layer. So, each neuron in this layer will learn a weight for the incoming value from the previous layer. So, for example, the if the first neuron in this layer is N21, and the neurons output from the previous layers are N11-N120, then this neuron will have 20 weights (W1-W20) and it will calculate its output to be:\n",
        "\n",
        "W1N11+W2N12+W3N13+...+W20N120+Bias\n",
        "\n",
        "So each of these weights will be learned as will the bias, for every neuron.\n",
        "\n",
        "Note that N11 refers to Layer 1 Neuron 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Reyw9wC65o8z"
      },
      "outputs": [],
      "source": [
        "print(layer_2.get_weights()[1].size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS89WZag7GlB"
      },
      "source": [
        "...and as expected there are 10 elements in this array, representing the 10 biases for the 10 neurons.\n",
        "\n",
        "Hopefully this helps you see how the element of a simple neuron containing y=mx+c can be expanded greatly into a deep neural network, and that DNN can learn the parameters that match the 784 pixels of an image to their output!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
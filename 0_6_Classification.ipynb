{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsarfraz/Tiny-Machine-Learning/blob/main/0_6_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "In this notebook I will be talking about classification which is a supervised machine learning method where a model tries to predict the correct label/category of input data (images). In classification, a model is fully trained using training data and is then evaluated on test data to perform predictions on new unseen data.\n",
        "\n",
        "## Re-cap: Dataset types in ML model training:\n",
        "To re-cap from [notebook 0.4](https://github.com/hsarfraz/Tiny-Machine-Learning/blob/main/0_4_Graphing_Neural_Network_Accuracy.ipynb), here are definitions of the different types of datasets used during machine learning (ML) model training:\n",
        "\n",
        " Types of ML Datasets  | Definition\n",
        " ------------- | -------------\n",
        " Training Dataset  | The dataset that is initially used to fit/train the ML model\n",
        " Validation Dataset  | The dataset used during the training phase of the model and used to fine-tune the model's parameters and make it accurate  \n",
        " Test Dataset  | The dataset used after the ML model has been fully trained to assess the model's perfomance on completly new/unseen data\n",
        "\n",
        " *  **NOTE:** Using a validation dataset is optional when training a ML model, but it is highly recommended because it improves the models accuracy. Some experts say that a validation dataset is not required if the ML model has no hyperparameters (ML models that don't have tuning options)\n",
        "\n",
        "## Overview of Artificial Intelligence, Machine Learning, and Deep Learning:\n",
        "\n",
        "Classification is a supervised learning method. Supervised learning is a subcategory of artificial intelligence and machine learning. Before I start talking about classification in greater detail I wanted to include a diagram which highlights the difference between artificial intelligence, machine learning, and deep learning.\n",
        "\n",
        "<img src=\"images/0.6_AI_ML_DL_Overview.jpg\" width=\"700\">\n",
        "\n",
        "\n",
        "### Defining Supervised Learning, Classification, and Regression\n",
        "\n",
        "Here is a diagram which provides a detailed explanation of supervised learning (I will define the other machine learning methods in future notebooks):\n",
        "\n",
        "<img src=\"images/0.6_supervised_learning.jpg\" width=\"700\">\n",
        "\n",
        "### Connection between Supervised Learning and Deep Learning\n",
        "\n",
        "*  Neural Networks:\n",
        "  *  Neural networks are used in deep learning algorithms and they train data by mimicking the connection of nodes in the human brain ([notebook 0.3](https://github.com/hsarfraz/Tiny-Machine-Learning/blob/main/0_3_Neural_Network_Basics.ipynb) includes a illustration of a neural net)\n",
        "  *  To re-cap from [notebook 0.5](https://github.com/hsarfraz/Tiny-Machine-Learning/blob/main/0_5_weights_and_bias_in_neural_network.ipynb) each node takes in inputs, weights, a bias, and produces a output. If the output values exceeds the given threshold value (this is established by the activation function), then the node is activated or \"fires\" and passes the output to the next layer of the network.\n",
        "  *  Neural Networks learn this process through supervised learning (the process that I am referring to is how each node uses greadient decent to minimize the loss function with the use of inputs, weights, and bias). When the loss functions result is at or near zero (it has a slope of zero) then we can be confident in the model's accuracy to produce a correct output/prediction of the input data.\n",
        "\n",
        "### Difference of Classification and Regression Neural Network Architecture\n",
        "\n",
        "I have included another image which highlights the difference between the neural network architecture of a classification ML model and Regression model. The methodology of having multiple neurons in the output layer is often used in classification and the having one neuron in the output layer is often used in regression.\n",
        "\n",
        "<img src=\"images/0.6_classification_regression_NN_architecture.jpg\" width=\"700\">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MxCvsDBbDlAF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL2nKxjS4S_r"
      },
      "source": [
        "# Explaining the Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dpkllLt9knmt"
      },
      "outputs": [],
      "source": [
        "# Load the necessary libraries\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "import numpy\n",
        "\n",
        "# Checking to see if TensorFlow 2 and Python 3 since this script requires TensorFlow 2 and Python 3.\n",
        "if sys.version_info.major < 3:\n",
        "    raise Exception((f\"The script is developed and tested for Python 3. \"\n",
        "                     f\"Current version: {sys.version_info.major}\"))\n",
        "\n",
        "if tf.__version__.split('.')[0] != '2':\n",
        "    raise Exception((f\"The script is developed and tested for tensorflow 2. \"\n",
        "                     f\"Current version: {tf.__version__}\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the dataset `MNIST` and normalizing the images (Feature Scaling)\n",
        "\n",
        "The `MNIST` dataset is already built into Tensorflow. It is a dataset of images that are labelled from 0-9. The dataset is split into training images (60,000) and labels, as well as validation images (10,000) and labels that is used for testing/validation.\n",
        "\n",
        "### What is Feature Scaling?\n",
        "\n",
        "Feature scaling is a method used to normalise a range of independent variables or features in a dataset. Feature scaling is also known as data normalisation and is performed during the data preprocessing stage (which is an iterative process where raw data is transformed and cleaned into a usable format). For example, if you have two independent variables with different ranges (ex. salary and age) you would want to apply feature scaling to these variables so they can all be on the same range (such as being centred around 0 or from 0-1). The type of range in which you want to place the data depends on the scaling technique used. The two most common feature scaling techniques are **normalisation and standardisation** which I will define below:\n",
        "\n",
        "* **Normalisation (aka min-max scaling or min-max normalization)**: It is the simplest feature scaling method and rescales the independent variable feature range to [0,1].\n",
        "  * The general normalisation formula is given below (the **max(x)** and **min(x)** are the feature's maximum and minimum values):\n",
        "  $$\n",
        "  x'=\\dfrac{x-min(x)}{max(x)-min(x)}\n",
        "  $$\n",
        "  * If you would like to would like to specify your interval to be in any [a,b] range then you can specify the range using the formula below:\n",
        "  $$\n",
        "  x'=a+\\dfrac{(x-min(x))(b-a)}{max(x)-min(x)}\n",
        "  $$\n",
        "* **Standardisation (aka feature standardisation)**: Makes the data values, in each feature, have a mean of 0 and unit variance. The formula uses the features distributed mean and standard deviation and calculates the new data point with this formula ( $\\overline{x}$ is the average of the feature vector and $\\sigma$ is the standard deviation of the feature vector):\n",
        "\n",
        " $$\n",
        " xâ€™ = \\dfrac{x-\\overline{x}}{\\sigma}\n",
        " $$\n",
        "\n",
        "### When to use normalisation or standardisation?\n",
        "\n",
        "When building a machine learning pipeline it is important to consider whether you are going to normalise or standardise your dataset features. I have highlighted some situations when it is best to use each feature scaling method.\n",
        "\n",
        "* **Normalisation**: Used when the data distribution does not follow the Gaussian distribution. It is also useful in algorithms that do not assume the data distribution like K-Nearest Neighbors (a classification algorithm which is under supervised learning). Another popular use case of normalisation is when an algorithm requires the data to be on a 0-1 scale. For example, in image processing it is essential to normalise the pixel intensity to fit within a certain range (the range of pixels is from 0-255 for the RGB colour range)\n",
        "\n",
        "* **Standardisation**: Used when the data follows a Gaussian Distribution (although this is not always the case). Since standardisation does not have a bounding range any outlets in the data will not be affected by standardisation. Standardisation is useful in clustering analysis (an unsupervised learning algorithm), to compare the similarities found between features based on certain distance measures. Another use case is during principal components analysis when individuals are interested in the components that maximise the data variance (something which min-max scaling does not present)\n",
        "\n"
      ],
      "metadata": {
        "id": "KPU2DpkbvvQM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### LOADING MNIST DATASET\n",
        "data = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (val_images, val_labels) = data.load_data()\n",
        "\n",
        "print(\"NOTE: I am printing out the max. and min. range of each image to show why I am dividing each image pixel by 255 for normalisation\")\n",
        "print()\n",
        "\n",
        "### GETTING THE MAX AND MIN PIXCEL VALUES WITHOUT USING NUMPY (SLICING METHOD)\n",
        "print(\"\\033[1mFinding the max and min pixel using slicing\\033[0m\")\n",
        "print(\"Here is the minimum pixel value in the training image dataset: \", training_images[..., 1].min())\n",
        "print(\"Here is the maximum pixel value in the training image dataset: \", training_images[..., 1].max())\n",
        "\n",
        "print()\n",
        "### GETTING THE MAX AND MIN PIXCEL VALUES USING NUMPY (NUMPY METHOD)\n",
        "print(\"\\033[1mFinding the max and min pixel using numpy\\033[0m\")\n",
        "print(\"Here is the minimum pixel value in the training image dataset: \", numpy.amin(training_images))\n",
        "print(\"Here is the minimum pixel value in the training image dataset: \", numpy.amax(training_images))\n",
        "\n",
        "### FEATURE SCALING (NORMALIZATION)\n",
        "training_images  = training_images / 255.0\n",
        "val_images = val_images / 255.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yG2dJWhpv3BX",
        "outputId": "afad5840-8c22-4071-ef73-8f716b3470c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n",
            "NOTE: I am printing out the max. and min. range of each image to show why I am dividing each image pixel by 255 for normalisation\n",
            "\n",
            "\u001b[1mFinding the max and min pixel using slicing\u001b[0m\n",
            "Here is the minimum pixel value in the training image dataset:  0\n",
            "Here is the maximum pixel value in the training image dataset:  255\n",
            "\n",
            "\u001b[1mFinding the max and min pixel using numpy\u001b[0m\n",
            "Here is the minimum pixel value in the training image dataset:  0\n",
            "Here is the minimum pixel value in the training image dataset:  255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining the Neural Network Model\n",
        "\n",
        "The code below defines the neural network that will be used to categorize the MNIST images.\n",
        "\n",
        "```python\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "```\n",
        "\n",
        "*  `tf.keras.models.Sequential([])` defines a sequential neural network. A sequential model consists of a linear stack of layers which are added to the model in sequential order. Each layer's output is fed as input to the next layer.\n",
        "*  Each layer of the neural network is defined in the square brackets.\n",
        "  *  `tf.keras.layers.Flatten(input_shape=(28,28))` is an operation that flattens out the images so they can be fed into the neural network. The flattened layer is added because the images in the MNIST dataset are 28 by 28, but the next layer has 20 neurons (a dimension of 20 by 1) so the dimension of the image needs to be something by 1 in order to fit in the layer. Flattening the image transforms its dimension from 28 by 28 to 784 by 1. `input_shape=(28,28)` tells the flattened layer to only take in images that have an input shape of 28 by 28. If it gets any other image shape an error will occur.\n",
        "  *  `tf.keras.layers.Dense(20, activation=tf.nn.relu)` is a dense layer which means that all neurons in this layer will get their source of input data from all the neurons in the previous layer. There are 20 neurons in this layer and it has the relu activation function. It is important to make sure that the correct number of neurons are added to the layer. If you assign too many neurons in the layer you can overspecialize or your neural network could be slow to learn. If the number of neurons is too small then your neural network wonâ€™t have enough neurons to learn the contents of the image (I will explore methodologies on how to pick the optimal number of neurons later)\n",
        "  *  `tf.keras.layers.Dense(10, activation=tf.nn.softmax)` is the final dense layer which has 10 neurons because the MNIST dataset has 10 classes (the digits from 0 to 9). This layer has the softmax activation function\n",
        "\n",
        "I have included a image below which illustrates the neural network architecture:\n",
        "\n",
        "<img src=\"images/0.6_MNIST_neural_net_architecture.jpg\" width=\"700\">\n",
        "\n",
        "## Defining the relu and softmax activation functions\n",
        "\n",
        "* **ReLU (rectified linear unit) activation function**: Every neuron will call its activation function assigned to each layer. The ReLU activation function changes any neuron output that is less than 0 to 0. So when $y=mx+b$ is evaluated on any of the neurons, in the first layer, the values that are less than zero will be thrown away and will be converted to zero. The ReLU activation function is commonly used in dense layers so the neurons donâ€™t cancel each other out. For example, if we did not have the ReLU activation function and one neuron returns a value of 10 and the other returns a value of -10 then both values will cancel each other out. The ReLU activation function establishes a non-linear relationship between the layers which can help capture complex relationships between the layers.\n",
        "* **softmax activation function**: The softmax activation function is called in the last layer and will help find the neuron, from the 10, that has the highest value."
      ],
      "metadata": {
        "id": "qNL05RtntkOp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zrQ-DeQtybXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1c9ad3-c006-4e73-dc3f-402e792e0d89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 11s 4ms/step - loss: 0.4157 - accuracy: 0.8817 - val_loss: 0.2737 - val_accuracy: 0.9211\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2511 - accuracy: 0.9284 - val_loss: 0.2269 - val_accuracy: 0.9344\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2111 - accuracy: 0.9398 - val_loss: 0.2061 - val_accuracy: 0.9426\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1835 - accuracy: 0.9477 - val_loss: 0.1818 - val_accuracy: 0.9484\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1643 - accuracy: 0.9528 - val_loss: 0.1730 - val_accuracy: 0.9505\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1503 - accuracy: 0.9563 - val_loss: 0.1586 - val_accuracy: 0.9524\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1390 - accuracy: 0.9597 - val_loss: 0.1623 - val_accuracy: 0.9530\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1304 - accuracy: 0.9617 - val_loss: 0.1591 - val_accuracy: 0.9539\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1236 - accuracy: 0.9633 - val_loss: 0.1483 - val_accuracy: 0.9568\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1173 - accuracy: 0.9653 - val_loss: 0.1507 - val_accuracy: 0.9576\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1121 - accuracy: 0.9664 - val_loss: 0.1466 - val_accuracy: 0.9581\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1080 - accuracy: 0.9680 - val_loss: 0.1481 - val_accuracy: 0.9583\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1032 - accuracy: 0.9693 - val_loss: 0.1536 - val_accuracy: 0.9569\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0999 - accuracy: 0.9702 - val_loss: 0.1467 - val_accuracy: 0.9590\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0972 - accuracy: 0.9706 - val_loss: 0.1477 - val_accuracy: 0.9579\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0932 - accuracy: 0.9713 - val_loss: 0.1501 - val_accuracy: 0.9569\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0905 - accuracy: 0.9720 - val_loss: 0.1464 - val_accuracy: 0.9593\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0877 - accuracy: 0.9729 - val_loss: 0.1520 - val_accuracy: 0.9574\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0858 - accuracy: 0.9738 - val_loss: 0.1484 - val_accuracy: 0.9599\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0828 - accuracy: 0.9747 - val_loss: 0.1509 - val_accuracy: 0.9592\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b3e27f5bfa0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    tf.keras.layers.Dense(20, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#to hide epoch output add `verbose=0`\n",
        "model.fit(training_images, training_labels, epochs=20, validation_data=(val_images, val_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN92xbwK4i_S"
      },
      "source": [
        "## Examine the test data\n",
        "\n",
        "Using model.evaluate, you can get metrics for a test set. In this case we only have a training set and a validation set, so we can try it out with the validation set. The accuracy will be slightly lower, at maybe 96%.  This is because the model hasn't previously seen this data and may not be fully generalized for all data. Still it's a pretty good score.\n",
        "\n",
        "You can also predict images, and compare against their actual label. The [0] image in the set is a number 7, and here you can see that neuron 7 has a 9.9e-1 (99%+) probability, so it got it right!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Rzit5Te-4lT6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9910b00b-d27f-475e-d1a5-47b5347b366a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1509 - accuracy: 0.9592\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "probability of neuron 1 : 1.2636455e-09\n",
            "probability of neuron 2 : 7.0626794e-12\n",
            "probability of neuron 3 : 8.122745e-05\n",
            "probability of neuron 4 : 0.0001044048\n",
            "probability of neuron 5 : 4.7030657e-11\n",
            "probability of neuron 6 : 1.1008105e-08\n",
            "probability of neuron 7 : 9.04926e-15\n",
            "probability of neuron 8 : 0.999806\n",
            "probability of neuron 9 : 6.816234e-06\n",
            "probability of neuron 10 : 1.4793163e-06\n",
            "\n",
            "neuron 7 has the highest probability of 9.04926e-15\n"
          ]
        }
      ],
      "source": [
        "model.evaluate(val_images, val_labels)\n",
        "\n",
        "classifications = model.predict(val_images)\n",
        "\n",
        "count = 0\n",
        "for x in classifications[0]:\n",
        "  count += 1\n",
        "  print('probability of neuron', count, ':' , x)\n",
        "\n",
        "#print(classifications[0])\n",
        "print()\n",
        "print('neuron',val_labels[0],'has the highest probability of',classifications[0][6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LkJGAiI5Cr3"
      },
      "source": [
        "## Modify to inspect learned values\n",
        "\n",
        "This code is identical, except that the layers are named prior to adding to the sequential. This allows us to inspect their learned parameters later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "eyyJ3RMYpFXR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccd5e5c1-6c4b-4f62-a518-0f290050bd57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.4066 - accuracy: 0.8877\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2349 - accuracy: 0.9324\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1989 - accuracy: 0.9431\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1759 - accuracy: 0.9495\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1597 - accuracy: 0.9539\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1477 - accuracy: 0.9572\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1377 - accuracy: 0.9604\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1292 - accuracy: 0.9626\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1228 - accuracy: 0.9642\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1164 - accuracy: 0.9666\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1124 - accuracy: 0.9674\n",
            "Epoch 12/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1072 - accuracy: 0.9681\n",
            "Epoch 13/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.1033 - accuracy: 0.9703\n",
            "Epoch 14/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0991 - accuracy: 0.9708\n",
            "Epoch 15/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0966 - accuracy: 0.9719\n",
            "Epoch 16/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0931 - accuracy: 0.9723\n",
            "Epoch 17/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0899 - accuracy: 0.9726\n",
            "Epoch 18/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0875 - accuracy: 0.9737\n",
            "Epoch 19/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0852 - accuracy: 0.9743\n",
            "Epoch 20/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0828 - accuracy: 0.9753\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.1433 - accuracy: 0.9599\n",
            "313/313 [==============================] - 0s 1ms/step\n",
            "[2.7122602e-05 2.6078533e-14 3.7789363e-05 1.8165493e-03 8.4196747e-12\n",
            " 3.1767691e-08 1.2280890e-16 9.9811757e-01 2.6253525e-07 7.1696292e-07]\n",
            "7\n"
          ]
        }
      ],
      "source": [
        "data = tf.keras.datasets.mnist\n",
        "\n",
        "(training_images, training_labels), (val_images, val_labels) = data.load_data()\n",
        "\n",
        "training_images  = training_images / 255.0\n",
        "val_images = val_images / 255.0\n",
        "layer_1 = tf.keras.layers.Dense(20, activation=tf.nn.relu)\n",
        "layer_2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape=(28,28)),\n",
        "                                    layer_1,\n",
        "                                    layer_2])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=20)\n",
        "\n",
        "model.evaluate(val_images, val_labels)\n",
        "\n",
        "classifications = model.predict(val_images)\n",
        "print(classifications[0])\n",
        "print(val_labels[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pNYFF935PlE"
      },
      "source": [
        "# Inspect weights\n",
        "\n",
        "If you print layer_1.get_weights(), you'll see a lot of data. Let's unpack it. First, there are 2 arrays in the result, so let's look at the first one. In particular let's look at its size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QACivjNKxFWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95b8433d-2f6c-4a23-d91e-326b216c67d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of the first array of the neural network (the first layer): 15680\n"
          ]
        }
      ],
      "source": [
        "print('size of the first array of the neural network (the first layer):',layer_1.get_weights()[0].size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqpHrDyp5acs"
      },
      "source": [
        "The above code should print 15680. Why?\n",
        "\n",
        "Recall that there are 20 neurons in the first layer.\n",
        "\n",
        "Recall also that the images are 28x28, which is 784.\n",
        "\n",
        "If you multiply 784 x 20 you get 15680.\n",
        "\n",
        "So...this layer has 20 neurons, and each neuron learns a W parameter for each pixel. So instead of y=Mx+c, we have\n",
        "y=M1X1+M2X2+M3X3+....+M784X784+C in every neuron!\n",
        "\n",
        "Every pixel has a weight in every neuron. Those weights are multiplied by the pixel value, summed up, and given a bias.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TdXrHDEw6ACm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1c8a763-5207-48ce-c0f4-37e708d4f1db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of neurons in the first layer of the neural network: 20\n"
          ]
        }
      ],
      "source": [
        "print('number of neurons in the first layer of the neural network:',layer_1.get_weights()[1].size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIOZ7rJy6Eg1"
      },
      "source": [
        "The above code will give you 20 -- the get_weights()[1] contains the biases for each of the 20 neurons in this layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyFKpzxN6T-N"
      },
      "source": [
        "## Inspecting layer 2\n",
        "\n",
        "Now let's look at layer 2. Printing the get_weights will give us 2 lists, the first a list of weights for the 10 neurons, and the second a list of biases for the 10 neurons\n",
        "\n",
        "Let's look first at the weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "o9P_PVwXyKXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ced5da98-3d26-47b4-fc4f-0ffad10312fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array size of the second layer in the neural network: 200\n"
          ]
        }
      ],
      "source": [
        "print('array size of the second layer in the neural network:',layer_2.get_weights()[0].size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daah2gq56fdb"
      },
      "source": [
        "This should return 200. Again, consider why?\n",
        "\n",
        "There are 10 neurons in this layer, but there are 20 neurons in the previous layer. So, each neuron in this layer will learn a weight for the incoming value from the previous layer. So, for example, the if the first neuron in this layer is N21, and the neurons output from the previous layers are N11-N120, then this neuron will have 20 weights (W1-W20) and it will calculate its output to be:\n",
        "\n",
        "W1N11+W2N12+W3N13+...+W20N120+Bias\n",
        "\n",
        "So each of these weights will be learned as will the bias, for every neuron.\n",
        "\n",
        "Note that N11 refers to Layer 1 Neuron 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Reyw9wC65o8z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dcc99ef-7ba2-4407-fa89-0d9520d4387d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of neurons in the second layer of the neural network: 10\n"
          ]
        }
      ],
      "source": [
        "print('number of neurons in the second layer of the neural network:',layer_2.get_weights()[1].size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DS89WZag7GlB"
      },
      "source": [
        "...and as expected there are 10 elements in this array, representing the 10 biases for the 10 neurons.\n",
        "\n",
        "Hopefully this helps you see how the element of a simple neuron containing y=mx+c can be expanded greatly into a deep neural network, and that DNN can learn the parameters that match the 784 pixels of an image to their output!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}